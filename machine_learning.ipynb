{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6O4fOh5qvFh",
        "outputId": "67a2adb3-b63d-4e6e-d3e7-7213b8d749b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-AXnPrK5TM9",
        "outputId": "3f573a41-24b4-40cf-da41-3866722397e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=51710715ab7f10c568b5b2398e0285cd0ca1fe6d0a2ddd0e6c0b516145054509\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#path = '/content/drive/MyDrive/Big Data/'\n",
        "\n",
        "import os\n",
        "path = \"drive/MyDrive/Ciclos/2023-1/Big Data/ProyectoBD/Big Data/\"\n",
        "print(os.listdir(path))"
      ],
      "metadata": {
        "id": "wZ-bAA2lsxA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6a4954-11a2-421b-be63-7a6331bf6bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['COVID19_Tweets_Dataset_Pq2', 'COVID19_Tweets_Dataset_Parquet', 'COVID19_Tweets_Dataset_Pq', 'COVID19_Tweets_Dataset_Pq3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "parte = \"00\"\n",
        "dia = \"01\"\n",
        "\n",
        "parte_max = 24\n",
        "dia_max = 2\n",
        "\n",
        "tables = [\"Summary_Details\", \"Summary_Hashtag\", \"Summary_Mentions\", \"Summary_NER\", \"Summary_NER_ES\", \"Summary_Sentiment\", \"Summary_Sentiment_ES\"]\n",
        "# tables = [\"Summary_NER_ES\"]\n",
        "\n",
        "for table in tables:\n",
        "  for d in range(1, dia_max):\n",
        "    for p in range(0, parte_max):\n",
        "      try:\n",
        "        if d < 10:\n",
        "          dia = \"0\"+str(d)\n",
        "        else:\n",
        "          dia = str(d)\n",
        "        if p < 10:\n",
        "          parte = \"0\"+str(p)\n",
        "        else:\n",
        "          parte = str(p)\n",
        "\n",
        "        if table == \"Summary_NER_ES\":\n",
        "          file = \"2022_01_\" + dia + \"_\" + parte + \"_Summary_ES_NER.csv\"\n",
        "        else:\n",
        "          file = \"2022_01_\" + dia + \"_\" + parte + \"_\" + table + \".csv\"\n",
        "\n",
        "        df = pd.read_csv(\"https://raw.githubusercontent.com/lopezbec/COVID19_Tweets_Dataset/main/\" + table + \"/2022_01/\" + file, dtype={'Country': str})\n",
        "\n",
        "        if 'Country' in df:\n",
        "          df = df.drop('Country', axis=1)\n",
        "        df = df.dropna()\n",
        "        if 'Tweet_ID' in df:\n",
        "          df = df.astype({'Tweet_ID': str})\n",
        "        for col in df.columns:\n",
        "          df = df.rename(columns={col: col.replace(\" \", \"_\")})\n",
        "\n",
        "        if not os.path.exists(path + \"COVID19_Tweets_Dataset_Pq3/\" + table):\n",
        "          os.mkdir(path + \"COVID19_Tweets_Dataset_Pq3/\" + table)\n",
        "\n",
        "        df.to_parquet(\n",
        "          path=path+\"COVID19_Tweets_Dataset_Pq3/\" + table + \"/\" + f'{file[:-4]}.parquet',\n",
        "          engine='pyarrow',\n",
        "          compression='gzip',\n",
        "        )\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "# df = pd.read_csv(\"https://raw.githubusercontent.com/lopezbec/COVID19_Tweets_Dataset/main/Summary_Details/2022_01/2022_01_\" + dia + \"_\" + parte + \"_Summary_Details.csv\")\n",
        "# df\n"
      ],
      "metadata": {
        "id": "AoPDSCpbKdfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -r COVID19_Tweets_Dataset_Pq/Summary_Sentiment_ES drive/MyDrive/Big\\ Data\n",
        "# !cp -r COVID19_Tweets_Dataset_Pq/Summary_NER_ES drive/MyDrive/Big\\ Data"
      ],
      "metadata": {
        "id": "LV7FryGRvJnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Big Data/'"
      ],
      "metadata": {
        "id": "yPGihd0qwDOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1E-OLk2wgO-",
        "outputId": "94f41e27-62e1-486b-c5d2-71df8047f8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['COVID19_Tweets_Dataset_Parquet', 'COVID19_Tweets_Dataset_Pq', 'COVID19_Tweets_Dataset_Pq2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo Machine Learning v2**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CiU7BNTXn-XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(master=\"local[2]\")\n",
        "sc"
      ],
      "metadata": {
        "id": "8Oa9VgbQzNi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = \"drive/MyDrive/Ciclos/2023-1/Big Data/ProyectoBD/Big Data/\"\n",
        "print(os.listdir(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUSF3mSf2l_A",
        "outputId": "ed50a126-5a65-4e22-c517-e3d4652047c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['COVID19_Tweets_Dataset_Pq2', 'COVID19_Tweets_Dataset_Parquet', 'COVID19_Tweets_Dataset_Pq', 'COVID19_Tweets_Dataset_Pq3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "       .master(\"local[2]\") \\\n",
        "       .appName(\"test\") \\\n",
        "       .config(\"spark.driver.memory\", \"6g\")\\\n",
        "       .config(\"spark.executor.memory\", \"6g\")\\\n",
        "       .getOrCreate()\n",
        "\n",
        "summary_details_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq3/Summary_Details\")\n",
        "summary_hashtag_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq3/Summary_Hashtag\")\n",
        "summary_mentions_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq3/Summary_Mentions\")\n",
        "summary_sentiment_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq3/Summary_Sentiment\")\n",
        "summary_ner_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq3/Summary_NER\")\n",
        "summary_sentiment_es_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq3/Summary_Sentiment_ES\")\n",
        "summary_ner_es_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq3/Summary_NER_ES\")"
      ],
      "metadata": {
        "id": "-RvHsGuisq6d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "joined_df = summary_details_df.select(\"Tweet_ID\", \"Language\", \"Likes\", \"Retweets\") \\\n",
        "    .join(summary_sentiment_df.select(\"Tweet_ID\", \"Sentiment_Label\"), \"Tweet_ID\") \\\n",
        "    .join(summary_ner_df.select(\"Tweet_ID\", \"NER_text\", \"NER_Label\"), \"Tweet_ID\")"
      ],
      "metadata": {
        "id": "TrROaOQWvd6H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = joined_df.limit(10000)\n",
        "joined_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co0CzMayMw40",
        "outputId": "dd952ca7-96a8-4944-a35d-aea7c7088a10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Convertir las etiquetas de sentimiento a valores numéricos\n",
        "label_indexer = StringIndexer(inputCol=\"Sentiment_Label\", outputCol=\"label\")\n",
        "\n",
        "# Convertir las columnas categóricas en índices numéricos\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\").setHandleInvalid(\"keep\") for col in [\"Language\", \"NER_text\", \"NER_Label\"]]\n",
        "\n",
        "# Codificar los índices numéricos en representaciones \"one-hot\"\n",
        "encoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_encoded\") for col in [\"Language\", \"NER_text\", \"NER_Label\"]]\n",
        "\n",
        "# Seleccionar las características relevantes y combinarlas en una sola columna\n",
        "feature_cols = [\"Language_encoded\", \"NER_text_encoded\", \"Likes\", \"Retweets\", \"NER_Label_encoded\"]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "# Crear el pipeline de transformación de datos\n",
        "pipeline = Pipeline(stages=indexers + encoders + [assembler, label_indexer])\n",
        "\n",
        "# Aplicar el pipeline para transformar los datos\n",
        "transformed_data = pipeline.fit(joined_df).transform(joined_df)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "(training_data, testing_data) = transformed_data.randomSplit([0.7, 0.3], seed=123)"
      ],
      "metadata": {
        "id": "GYabNiBEvj9T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# Crear una instancia del clasificador\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
        "\n",
        "# Entrenar el modelo con los datos de entrenamiento\n",
        "model = rf.fit(training_data)"
      ],
      "metadata": {
        "id": "1Gds0HD-wBdD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar predicciones en el conjunto de prueba\n",
        "predictions = model.transform(testing_data)\n",
        "\n",
        "# Mostrar algunas predicciones\n",
        "predictions.select(\"Sentiment_Label\", \"prediction\", \"probability\").show(10)"
      ],
      "metadata": {
        "id": "FhuEvTh8wE_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a70604-8651-458e-e658-80b3c2184932"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+--------------------+\n",
            "|Sentiment_Label|prediction|         probability|\n",
            "+---------------+----------+--------------------+\n",
            "|        neutral|       1.0|[0.42244810571732...|\n",
            "|        neutral|       0.0|[0.45632943034949...|\n",
            "|        neutral|       1.0|[0.44624937856696...|\n",
            "|       negative|       1.0|[0.44624937856696...|\n",
            "|       negative|       1.0|[0.44624937856696...|\n",
            "|       positive|       1.0|[0.42244810571732...|\n",
            "|       negative|       0.0|[0.47787013761230...|\n",
            "|       negative|       1.0|[0.44624937856696...|\n",
            "|        neutral|       1.0|[0.42244810571732...|\n",
            "|       negative|       0.0|[0.45632943034949...|\n",
            "+---------------+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Evaluar el rendimiento del modelo en el conjunto de prueba\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "MTz2-NuOwHqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3670a1-da23-4db0-8048-fa5fa1122dd3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5117416829745597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo Machine Learning**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QX5X8l1P0YSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline"
      ],
      "metadata": {
        "id": "OrYlNqpk0Es3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Mi aplicacion PySpark\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "wn1ErteL0Is9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary_details = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq2/Summary_Details\")\n",
        "df_summary_hashtag = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq2/Summary_Hashtag\")\n",
        "df_summary_mentions = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq2/Summary_Mentions\")\n",
        "df_summary_ner = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq2/Summary_NER/\")\n",
        "# df_summary_ner_es = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq2/Summary_NER_ES\")\n",
        "df_summary_sentiment = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq2/Summary_Sentiment\")\n",
        "# df_summary_sentiment_es = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq2/Summary_Sentiment_ES\")"
      ],
      "metadata": {
        "id": "gAzXYBM_0JCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_summary_details)\n",
        "print(df_summary_hashtag)\n",
        "print(df_summary_mentions)\n",
        "print(df_summary_ner)\n",
        "# print(df_summary_ner_es)\n",
        "print(df_summary_sentiment)\n",
        "# print(df_summary_sentiment_es)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0urz2qFS0Onv",
        "outputId": "8d2f85a6-f896-494a-9397-aa817dda4845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[Tweet_ID: string, Language: string, Geolocation_coordinate: string, RT: string, Likes: bigint, Retweets: bigint, Date_Created: string]\n",
            "DataFrame[Tweet_ID: string, Hastag: string]\n",
            "DataFrame[Tweet_ID: string, Mentions: string]\n",
            "DataFrame[Tweet_ID: string, NER_Text: string, Start_Pos: bigint, Eng_Pos: bigint, NER_Label: string, Prob: double]\n",
            "DataFrame[Tweet_ID: string, Sentiment_Label: string, Logits_Neutral: double, Logits_Positive: double, Logits_Negative: double]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary_details.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c8NyAyyKATU",
        "outputId": "b3cb123c-8fe2-43b1-ffb8-d1c3ef11eea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Tweet_ID='1479105481037672457', Language='es', Geolocation_coordinate='NO', RT='YES', Likes=0, Retweets=1, Date_Created='Thu Jan 06 15:00:01 +0000 2022')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar un join entre los DataFrames\n",
        "df_join = df_summary_details.join(df_summary_hashtag, on='Tweet_ID')\n",
        "df_join = df_join.join(df_summary_ner, on='Tweet_ID')\n",
        "df_join = df_join.join(df_summary_sentiment, on='Tweet_ID')\n",
        "\n",
        "df_join.describe()\n",
        "# Seleccionar las columnas relevantes para la consulta\n",
        "# df_result = df_join.select('Tweet_ID', 'Language', 'Geolocation_coordinate', 'RT', 'Likes', 'Retweets',\n",
        "                          #  'Date_Created', 'Hashtag', 'NER_Text', 'Sentiment_Label')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQat8mQY3Cdl",
        "outputId": "bd3dc176-ba18-46e0-bbd7-1db176e5ddd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, Tweet_ID: string, Language: string, Geolocation_coordinate: string, RT: string, Likes: string, Retweets: string, Date_Created: string, Hastag: string, NER_Text: string, Start_Pos: string, Eng_Pos: string, NER_Label: string, Prob: string, Sentiment_Label: string, Logits_Neutral: string, Logits_Positive: string, Logits_Negative: string]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_join = df_join.select('Tweet_ID', 'Likes', 'Date_Created', 'Hastag', 'NER_Text', 'Sentiment_Label')"
      ],
      "metadata": {
        "id": "0rYSnlj55e-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af83057-acb9-48fe-a26b-9bbae7e86040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(Tweet_ID='1477067087365423104', Likes=847, Date_Created='Sat Jan 01 00:00:10 +0000 2022', Hastag='#Covid19', NER_Text='un', Sentiment_Label='neutral')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparación de datos\n",
        "# selected_df = df_summary_sentiment.select('Logits_Neutral', 'Logits_Positive', 'Logits_Negative', 'Sentiment_Label')\n",
        "label_indexer = StringIndexer(inputCol='Sentiment_Label', outputCol='label')\n",
        "indexed_df = label_indexer.fit(df_join).transform(df_join)\n",
        "# indexed_df = label_indexer.fit(selected_df).transform(selected_df)\n",
        "# indexed_df.head(20)"
      ],
      "metadata": {
        "id": "NhUkmdNZ0jfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación de conjunto de entrenamiento y prueba\n",
        "train_data, test_data = indexed_df.randomSplit([0.7, 0.3])"
      ],
      "metadata": {
        "id": "_teCj3iw0qXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construcción del pipeline\n",
        "# assembler = VectorAssembler(inputCols=['Logits_Neutral', 'Logits_Positive', 'Logits_Negative'], outputCol='features')\n",
        "assembler = VectorAssembler(inputCols=['Likes'], outputCol='features')\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
        "pipeline = Pipeline(stages=[assembler, lr])"
      ],
      "metadata": {
        "id": "kIwqxgkP0xH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajuste del modelo\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Evaluación del modelo\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluar las predicciones utilizando métricas de clasificación\n"
      ],
      "metadata": {
        "id": "OCUJQb0N01WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realización de predicciones en nuevos datos\n",
        "new_data = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq2/Summary_Details/2022_01_01_01_Summary_Details.parquet\")\n",
        "new_predictions = model.transform(new_data)\n",
        "new_predictions.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUgxf3nv068n",
        "outputId": "3ca93ec0-666c-4179-c71e-046a1c690bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Tweet_ID='1477090842997313539', Language='en', Geolocation_coordinate='NO', RT='YES', Likes=0, Retweets=4, Date_Created='Sat Jan 01 01:34:34 +0000 2022', features=DenseVector([0.0]), rawPrediction=DenseVector([0.5337, 0.2632, -0.7969]), probability=DenseVector([0.4932, 0.3764, 0.1304]), prediction=0.0),\n",
              " Row(Tweet_ID='1477093031853510657', Language='und', Geolocation_coordinate='NO', RT='NO', Likes=0, Retweets=0, Date_Created='Sat Jan 01 01:43:16 +0000 2022', features=DenseVector([0.0]), rawPrediction=DenseVector([0.5337, 0.2632, -0.7969]), probability=DenseVector([0.4932, 0.3764, 0.1304]), prediction=0.0),\n",
              " Row(Tweet_ID='1477082164919521283', Language='fr', Geolocation_coordinate='NO', RT='YES', Likes=0, Retweets=79, Date_Created='Sat Jan 01 01:00:05 +0000 2022', features=DenseVector([0.0]), rawPrediction=DenseVector([0.5337, 0.2632, -0.7969]), probability=DenseVector([0.4932, 0.3764, 0.1304]), prediction=0.0),\n",
              " Row(Tweet_ID='1477082188248190984', Language='en', Geolocation_coordinate='NO', RT='YES', Likes=0, Retweets=43, Date_Created='Sat Jan 01 01:00:11 +0000 2022', features=DenseVector([0.0]), rawPrediction=DenseVector([0.5337, 0.2632, -0.7969]), probability=DenseVector([0.4932, 0.3764, 0.1304]), prediction=0.0),\n",
              " Row(Tweet_ID='1477082212008968193', Language='fr', Geolocation_coordinate='NO', RT='NO', Likes=0, Retweets=0, Date_Created='Sat Jan 01 01:00:16 +0000 2022', features=DenseVector([0.0]), rawPrediction=DenseVector([0.5337, 0.2632, -0.7969]), probability=DenseVector([0.4932, 0.3764, 0.1304]), prediction=0.0),\n",
              " Row(Tweet_ID='1477082272574558212', Language='en', Geolocation_coordinate='NO', RT='NO', Likes=0, Retweets=0, Date_Created='Sat Jan 01 01:00:31 +0000 2022', features=DenseVector([0.0]), rawPrediction=DenseVector([0.5337, 0.2632, -0.7969]), probability=DenseVector([0.4932, 0.3764, 0.1304]), prediction=0.0),\n",
              " Row(Tweet_ID='1477082297056874498', Language='fr', Geolocation_coordinate='NO', RT='YES', Likes=0, Retweets=310, Date_Created='Sat Jan 01 01:00:37 +0000 2022', features=DenseVector([0.0]), rawPrediction=DenseVector([0.5337, 0.2632, -0.7969]), probability=DenseVector([0.4932, 0.3764, 0.1304]), prediction=0.0),\n",
              " Row(Tweet_ID='1477082305004912641', Language='en', Geolocation_coordinate='NO', RT='NO', Likes=0, Retweets=0, Date_Created='Sat Jan 01 01:00:39 +0000 2022', features=DenseVector([0.0]), rawPrediction=DenseVector([0.5337, 0.2632, -0.7969]), probability=DenseVector([0.4932, 0.3764, 0.1304]), prediction=0.0),\n",
              " Row(Tweet_ID='1477082308066881538', Language='en', Geolocation_coordinate='NO', RT='NO', Likes=4, Retweets=0, Date_Created='Sat Jan 01 01:00:39 +0000 2022', features=DenseVector([4.0]), rawPrediction=DenseVector([0.5323, 0.2638, -0.7961]), probability=DenseVector([0.4927, 0.3767, 0.1305]), prediction=0.0),\n",
              " Row(Tweet_ID='1477082311707537408', Language='fr', Geolocation_coordinate='NO', RT='YES', Likes=0, Retweets=279, Date_Created='Sat Jan 01 01:00:40 +0000 2022', features=DenseVector([0.0]), rawPrediction=DenseVector([0.5337, 0.2632, -0.7969]), probability=DenseVector([0.4932, 0.3764, 0.1304]), prediction=0.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}