{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "El entrenamiento y evaluación de los modelos de Machine Learning se realiza en Google Colab con un subconjunto del dataset."
      ],
      "metadata": {
        "id": "eWBseB1iliOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar e importar librerías útiles\n",
        "!pip install pyspark\n",
        "!pip install pandas\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Montar Google Drive en Colab (opcional)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Elegir la ruta en la que se almacenan los datos\n",
        "path = \"drive/MyDrive/Ciclos/2023-1/Big Data/ProyectoBD/Big Data/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6O4fOh5qvFh",
        "outputId": "8391f1ad-51dc-4fb9-9f5e-3edde3910070"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtención de datos (opcional)"
      ],
      "metadata": {
        "id": "QguZLrQ9lb-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido a que Colab no cuenta con los recursos para acceder y procesar el dataset completo, el siguiente script recopila desde GitHub un subconjunto de este conformado para una cantidad de días variable dentro del primer mes del año, por defecto solo se extraen los datos del primer día, es decir, solo se cuenta con información de tweets publicados el 01/01/2022. Si ya ejecutaste este script o cuentas con datos de otro medio, puedes saltar esta sección"
      ],
      "metadata": {
        "id": "e7CvCJ-gnXvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parte_max = 23 # Número máximo de particiones existentes en el dataset para los datos de un día, se define para obtener todos los datos de una cantidad de días específica (no modificar)\n",
        "dia_max = 1 # Número de días sobre los cuáles se desea extraer los datos por cada tabla (variable)\n",
        "\n",
        "# Constantes útiles para acceder a los archivos de las tablas\n",
        "parte = \"00\"\n",
        "dia = \"01\"\n",
        "\n",
        "# Lista de tablas del repositorio\n",
        "tables = [\"Summary_Details\", \"Summary_Hashtag\", \"Summary_Mentions\", \"Summary_NER\", \"Summary_NER_ES\", \"Summary_Sentiment\", \"Summary_Sentiment_ES\"]\n",
        "\n",
        "# Extraer archivos csv del github, renombrarlos y convertirlos en formato parquet (incluye pasos de preprocesamiento vistos anteriormente)\n",
        "for table in tables:\n",
        "  for d in range(1, dia_max+1):\n",
        "    for p in range(0, parte_max+1):\n",
        "      try:\n",
        "        if d < 10:\n",
        "          dia = \"0\"+str(d)\n",
        "        else:\n",
        "          dia = str(d)\n",
        "        if p < 10:\n",
        "          parte = \"0\"+str(p)\n",
        "        else:\n",
        "          parte = str(p)\n",
        "\n",
        "        if table == \"Summary_NER_ES\":\n",
        "          file = \"2022_01_\" + dia + \"_\" + parte + \"_Summary_ES_NER.csv\"\n",
        "        else:\n",
        "          file = \"2022_01_\" + dia + \"_\" + parte + \"_\" + table + \".csv\"\n",
        "\n",
        "        df = pd.read_csv(\"https://raw.githubusercontent.com/lopezbec/COVID19_Tweets_Dataset/main/\" + table + \"/2022_01/\" + file, dtype={'Tweet_ID': str})\n",
        "\n",
        "        if 'Country' in df:\n",
        "          df = df.drop('Country', axis=1)\n",
        "        df = df.dropna()\n",
        "\n",
        "        for col in df.columns:\n",
        "          df = df.rename(columns={col: col.replace(\" \", \"_\")})\n",
        "\n",
        "        if not os.path.exists(path + \"COVID19_Tweets_Dataset_Pq/\" + table):\n",
        "          os.mkdir(path + \"COVID19_Tweets_Dataset_Pq/\" + table)\n",
        "\n",
        "        df.to_parquet(\n",
        "          path=path+\"COVID19_Tweets_Dataset_Pq/\" + table + \"/\" + f'{file[:-4]}.parquet',\n",
        "          engine='pyarrow',\n",
        "          compression='gzip',\n",
        "        )\n",
        "      except:\n",
        "        continue"
      ],
      "metadata": {
        "id": "AoPDSCpbKdfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento de modelos"
      ],
      "metadata": {
        "id": "CiU7BNTXn-XG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar las predicciones entrenamos y evaluamos dos modelos de Machine Learning: Random Forest y Regresión Lineal."
      ],
      "metadata": {
        "id": "3VvfZGZi4vP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
        "\n",
        "# Iniciar sesión en spark\n",
        "spark = SparkSession.builder \\\n",
        "       .master(\"local[2]\") \\\n",
        "       .appName(\"test\") \\\n",
        "       .config(\"spark.driver.memory\", \"6g\")\\\n",
        "       .config(\"spark.executor.memory\", \"6g\")\\\n",
        "       .getOrCreate()"
      ],
      "metadata": {
        "id": "-RvHsGuisq6d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leer los archivos parquet\n",
        "summary_details_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq/Summary_Details\")\n",
        "summary_hashtag_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq/Summary_Hashtag\")\n",
        "summary_mentions_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq/Summary_Mentions\")\n",
        "summary_sentiment_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq/Summary_Sentiment\")\n",
        "summary_ner_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq/Summary_NER\")\n",
        "summary_sentiment_es_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq/Summary_Sentiment_ES\")\n",
        "summary_ner_es_df = spark.read.parquet(path + \"COVID19_Tweets_Dataset_Pq/Summary_NER_ES\")"
      ],
      "metadata": {
        "id": "9hlXtrH2czYw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar join de los dataframes en parquet (modificar en base a las columnas que se quiera usar)\n",
        "joined_df = summary_details_df.select(\"Tweet_ID\", \"Likes\", \"Retweets\") \\\n",
        "    .join(summary_sentiment_df.select(\"Tweet_ID\", \"Sentiment_Label\"), \"Tweet_ID\") \\\n",
        "    .join(summary_ner_df.select(\"Tweet_ID\", \"NER_text\", \"NER_Label\"), \"Tweet_ID\")\n",
        "\n",
        "# Extraer una cantidad de filas del dataframe (modificar en base a la cantidad con que se quiera trabajar)\n",
        "joined_df = joined_df.limit(10000)"
      ],
      "metadata": {
        "id": "TrROaOQWvd6H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir las etiquetas de sentimiento a valores numéricos\n",
        "label_indexer = StringIndexer(inputCol=\"Sentiment_Label\", outputCol=\"label\")\n",
        "\n",
        "# Convertir las columnas categóricas en índices numéricos\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\").setHandleInvalid(\"keep\") for col in [\"NER_text\", \"NER_Label\"]]\n",
        "\n",
        "# Codificar los índices numéricos en representaciones one-hot\n",
        "encoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_encoded\") for col in [\"NER_text\", \"NER_Label\"]]\n",
        "\n",
        "# Seleccionar las características relevantes y combinarlas en una sola columna\n",
        "feature_cols = [\"NER_text_encoded\", \"Likes\", \"Retweets\", \"NER_Label_encoded\"]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
      ],
      "metadata": {
        "id": "vF8BrqvAt77q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el pipeline de transformación de datos\n",
        "pipeline = Pipeline(stages=indexers + encoders + [assembler, label_indexer])\n",
        "\n",
        "# Aplicar el pipeline para transformar los datos\n",
        "transformed_data = pipeline.fit(joined_df).transform(joined_df)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "(training_data, testing_data) = transformed_data.randomSplit([0.7, 0.3], seed=138)"
      ],
      "metadata": {
        "id": "GYabNiBEvj9T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una instancia del clasificador RandomForest\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
        "\n",
        "# Crear el modelo de Regresión Logística\n",
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
        "\n",
        "# Entrenar los modelos con los datos de entrenamiento\n",
        "modelRF = rf.fit(training_data)\n",
        "modelLR = lr.fit(training_data)"
      ],
      "metadata": {
        "id": "1Gds0HD-wBdD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación de modelos"
      ],
      "metadata": {
        "id": "vpC0zogMl4vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "# Realizar predicciones en el conjunto de prueba con el modelo Random Forest\n",
        "predictionsRF = modelRF.transform(testing_data)"
      ],
      "metadata": {
        "id": "FhuEvTh8wE_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0cf0dd2-44b9-4b9a-c73b-3bdb66d3f418"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58.5 ms ± 9.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "# Realizar predicciones en el conjunto de prueba con el modelo de Regresión Lineal\n",
        "predictionsLR = modelLR.transform(testing_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6vIClaQ4WjN",
        "outputId": "6ea65266-8876-4cab-d60b-46bd609eb976"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70.5 ms ± 14.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar algunas predicciones\n",
        "predictionsRF = modelRF.transform(testing_data)\n",
        "predictionsLR = modelLR.transform(testing_data)\n",
        "predictionsRF.select(\"Sentiment_Label\", \"prediction\", \"probability\").show(10)\n",
        "predictionsLR.select(\"Sentiment_Label\", \"prediction\", \"probability\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7npZoRY4jTo",
        "outputId": "de81614a-2829-47b4-a453-ff342356741d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+--------------------+\n",
            "|Sentiment_Label|prediction|         probability|\n",
            "+---------------+----------+--------------------+\n",
            "|        neutral|       1.0|[0.45004128162225...|\n",
            "|        neutral|       1.0|[0.45004128162225...|\n",
            "|        neutral|       1.0|[0.45004128162225...|\n",
            "|        neutral|       1.0|[0.45004128162225...|\n",
            "|       negative|       1.0|[0.45004128162225...|\n",
            "|       negative|       1.0|[0.45004128162225...|\n",
            "|       positive|       1.0|[0.43014753646274...|\n",
            "|       positive|       1.0|[0.43014753646274...|\n",
            "|        neutral|       1.0|[0.45004128162225...|\n",
            "|        neutral|       1.0|[0.45004128162225...|\n",
            "+---------------+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+---------------+----------+--------------------+\n",
            "|Sentiment_Label|prediction|         probability|\n",
            "+---------------+----------+--------------------+\n",
            "|        neutral|       0.0|[0.45772565187304...|\n",
            "|        neutral|       0.0|[0.43585955343369...|\n",
            "|        neutral|       1.0|[0.35103240800729...|\n",
            "|        neutral|       0.0|[0.69437251065540...|\n",
            "|       negative|       1.0|[0.33572794164933...|\n",
            "|       negative|       1.0|[2.05405789788479...|\n",
            "|       positive|       2.0|[2.24557622450630...|\n",
            "|       positive|       0.0|[0.43507897780464...|\n",
            "|        neutral|       0.0|[0.58105570444736...|\n",
            "|        neutral|       1.0|[0.35570325903343...|\n",
            "+---------------+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el rendimiento del modelo en el conjunto de prueba\n",
        "evaluatorRF = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "evaluatorLR = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "accuracyRF = evaluatorRF.evaluate(predictionsRF)\n",
        "accuracyLR = evaluatorLR.evaluate(predictionsLR)\n",
        "print(\"Random Forest Accuracy:\", accuracyRF)\n",
        "print(\"Logistic Regression Accuracy:\", accuracyLR)"
      ],
      "metadata": {
        "id": "MTz2-NuOwHqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1414538a-7652-40a7-b89f-b8044c444409"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.49274406332453824\n",
            "Logistic Regression Accuracy: 0.7040684633296742\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}