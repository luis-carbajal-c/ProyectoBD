{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Obtención de datos"
      ],
      "metadata": {
        "id": "O6_oUXOsAMuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los datos se obtienen clonando el repositorio de COVID19_Tweets_Dataset dentro del nodo master en un directorio con suficiente almacenamiento (más de 120 GB), en nuestro caso el directorio /datadrive.\n",
        "\n",
        "```\n",
        "cd /datadrive\n",
        "git clone https://github.com/lopezbec/COVID19_Tweets_Dataset.git\n",
        "```\n",
        "\n",
        "Una vez hecho esto es necesario hacer una limpieza preliminar de los datos que consiste en simplemente remover aquellos directorios y archivos del dataset que no vamos a utilizar para que los scripts se ejecuten correctamente. Debemos remover las carpetas NYT_COVID_with_Reverse_Geo y Tweets_ID_Filter_requests así como todos los archivos restantes que no son carpetas, de modo que al final solo nos quedan las carpetas que corresponden a las siete tablas principales. Dentro de cada carpeta restante tenemos 12 carpetas que contienen la data de los 12 meses del 2022 además de archivos adicionales con análisis previos de las tablas, estos archivos deben ser removidos de modo que solo queden las carpetas.\n",
        "\n",
        "```\n",
        "rm archivo\n",
        "rm -r carpeta\n",
        "```"
      ],
      "metadata": {
        "id": "2LzzhfBUAQQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento de datos"
      ],
      "metadata": {
        "id": "ZMDbNtdb7FRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los scripts deben ser ejecutados desde el mismo directorio donde se clonó el dataset."
      ],
      "metadata": {
        "id": "l8dDCHssEpHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar de librerías necesarias\n",
        "!pip install pandas\n",
        "!pip install pyarrow\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "GOGzNG_I6s0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQANMTTa6Q3j"
      },
      "outputs": [],
      "source": [
        "# Importar dependencias\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import platform\n",
        "from glob import glob\n",
        "import os\n",
        "import shutil\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Definir la ruta del dataset\n",
        "path = os.getcwd() + \"/COVID19_Tweets_Dataset\"\n",
        "l = os.listdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mover los archivos de datos dentro de las carpetas de meses para ser accesibles directamente desde las carpetas principales de las tablas (solo se ejecuta una vez)\n",
        "for direc in l:\n",
        "    for mes in os.listdir(os.path.join(path, direc)):\n",
        "        if not mes.endswith(\"csv\"):\n",
        "            if not os.listdir(os.path.join(path, direc, mes)):\n",
        "                os.rmdir(os.path.join(path, direc, mes))\n",
        "            else:\n",
        "                for csv in os.listdir(os.path.join(path, direc, mes)):\n",
        "                    shutil.move(os.path.join(path, direc, mes, csv), os.path.join(path, direc))"
      ],
      "metadata": {
        "id": "ld6ml2YiK4bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapear cada archivo de datos con la tabla a la que pertenecen\n",
        "tables = {}\n",
        "for direct in l:\n",
        "    tables[direct] = []\n",
        "    for csv in os.listdir(os.path.join(path, direct)):\n",
        "        if csv.endswith(\".csv\"):\n",
        "            tables[direct].append(csv)\n",
        "\n",
        "print(tables['Summary_Details'])"
      ],
      "metadata": {
        "id": "l3kpObHQK52b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Análisis de valores nulos en las tablas (opcional y no recomendado sobre todo el dataset)\n",
        "for key in tables:\n",
        "    print(key)\n",
        "    for csv in tables[key]:\n",
        "        df = pd.read_csv(os.path.join(path, key, csv))\n",
        "        print(df.isnull().sum())\n",
        "        break"
      ],
      "metadata": {
        "id": "8BALVeBMK7cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesamiento de los datos\n",
        "for key in tables:\n",
        "    print(key)\n",
        "    for csv in tables[key]:\n",
        "        if not os.path.exists(os.getcwd() + \"/COVID19_Tweets_Dataset_Pq/\" + key + \"/\" + f'{csv[:-4]}.parquet'):\n",
        "            df = pd.read_csv(os.path.join(path, key, csv), dtype={'Country': str})\n",
        "\n",
        "            if key == 'Summary_Details':\n",
        "                # Remover la columna 'Country'\n",
        "                df = df.drop('Country', axis=1)\n",
        "\n",
        "            # Remover los NA\n",
        "            df = df.dropna()\n",
        "\n",
        "            if 'Tweet_ID' in df:\n",
        "                # Cambiar la variable Tweet_ID de numérica a categórica\n",
        "                df = df.astype({'Tweet_ID': str})\n",
        "\n",
        "            # Remover espacios en nombre de columnas\n",
        "            for col in df.columns:\n",
        "                df = df.rename(columns={col: col.replace(\" \", \"_\")})\n",
        "\n",
        "            if not os.path.exists(os.getcwd() + \"/COVID19_Tweets_Dataset_Pq/\" + key):\n",
        "                os.mkdir(os.getcwd() + \"/COVID19_Tweets_Dataset_Pq/\" + key)\n",
        "\n",
        "            # Convertir los dataframes a formato parquet\n",
        "            df.to_parquet(\n",
        "                path=os.getcwd() + \"/COVID19_Tweets_Dataset_Pq/\" + key + \"/\" + f'{csv[:-4]}.parquet',\n",
        "                engine='pyarrow',\n",
        "                compression='gzip',\n",
        "            )"
      ],
      "metadata": {
        "id": "gJRB8_iqK844"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lectura de datos parquet en el clúster HDFS"
      ],
      "metadata": {
        "id": "6hlcF6AP7LOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez haya finalizado el preprocesamiento, puedes subir los archivos en formato parquet al clúster con los siguientes comandos:\n",
        "\n",
        "```\n",
        "hdfs dfs -mkdir -p /user/hadoop\n",
        "hdfs dfs -mkdir -p datasets\n",
        "hdfs dfs -put COVID19_Tweets_Dataset_Pq datasets\n",
        "```\n",
        "\n",
        "Luego de finalizar la subida, puedes probar que los archivos se lean correctamente con los siguientes scripts:"
      ],
      "metadata": {
        "id": "buuUxPJTZ4di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"HDFS Parquet Test\").getOrCreate()"
      ],
      "metadata": {
        "id": "rlSJ0jbWLAFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leer tabla Summary_Details y contar filas\n",
        "df = spark.read.parquet(os.getcwd() + \"/COVID19_Tweets_Dataset_Pq/Summary_Details\")\n",
        "df.count()"
      ],
      "metadata": {
        "id": "cV8g4XVyLBhd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nXPet_Aps074",
        "_a1wFEles4Z-"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}